{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn') \n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import helper\n",
    "helper.ENCODER_BASE_NAME='stacked_autoencoder_tf2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The swaption volatility data are stored in an csv file in the 'input_data' directory. The helper function ``get_df_from_csv(filename)`` read this data and parse some auxillary data from the header.\n",
    "Note that in our git only an example data set can be found. The images shown in the following are build on the larger data set decripted in the paper. You are invited to use your own data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import get_df_from_csv\n",
    "dfInput=get_df_from_csv('EURSWVOLN.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the ``plot_swaption_volas(dfInput, trade_date)`` function you can plot all volatilities at given date, in the following example the 28.12.2018. (All plot functions save the images in then 'images' directory.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import plot_swaption_volas\n",
    "plot_swaption_volas(dfInput, '20181228')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or just the feature vector with ``plot_feature_vector(dfInput, trade_date)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import plot_feature_vector\n",
    "plot_feature_vector(dfInput, '20181228')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the paper we implanted anomalies at the 20Y-05Y-100bps point at 28.12.2018 an try to find them (at least the biggest ones) with the algorithm. The implanted data sets are saved at synthetic dates in 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import plot_swaption_volas2\n",
    "trade_dates=['18000101','18000102','18000103','18000104','18000105','20181228']\n",
    "labels=['20','10','5','3','2','original']\n",
    "optionPeriodSymbol='20Y'\n",
    "swapPeriodSymbol='05Y'\n",
    "plot_swaption_volas2(dfInput, trade_dates, labels, optionPeriodSymbol, swapPeriodSymbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an impression about the dimensionality of the problem. One way is to peforme an PCA (the special case of an autoencoder with linear activation function), which can easyly done with sklearn, and have a look at the sum of the variance of the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA()\n",
    "pca.fit(dfInput['data'].values)\n",
    "explained_variance_ratio_cumsum=pd.DataFrame(data=pca.explained_variance_ratio_.cumsum())\n",
    "print(explained_variance_ratio_cumsum[:15])\n",
    "explained_variance_ratio_cumsum.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with 5 principal components more then 91% of the variance are explained and with 10 more then 97%. In the following we autoencoder with 5 and 10 bottlenecks, with and without denoising and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bottlenecks=[5,5,10,10]\n",
    "noise_stddevs=[0,0.01,0,0.01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step we create an 90% train an 10% test set, where the inplanted data points (1800) are neglected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_synth=dfInput['data'][dfInput['data'].index<'1900']\n",
    "dataset_train=dfInput['data'][dfInput['data'].index>'1900'].sample(frac=0.9,random_state=42)\n",
    "dataset_test = dfInput['data'][dfInput['data'].index>'1900'].drop(dataset_train.index)\n",
    "print(\"test.shape: {}\".format(dataset_test.shape))\n",
    "print(\"train.shape: {}\".format(dataset_train.shape))\n",
    "print(\"synthetic.shape: {}\".format(dataset_synth.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralnetwork_tf2 import calibrate_stacked_autoencoder\n",
    "n_epoch=10000\n",
    "n_bottlenecks=[5,5,10,10]\n",
    "noise_stddevs=[0,0.01,0,0.01]\n",
    "for n_bottleneck, noise_stddev in zip (n_bottlenecks, noise_stddevs):    \n",
    "    input_train=dataset_train.values\n",
    "    input_test=dataset_test.values\n",
    "    calibrate_stacked_autoencoder(n_epoch, input_train=input_train, input_test=input_test, n_bottleneck=n_bottleneck, noise_stddev_=noise_stddev)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function ``get_prediction_from_model(dfInput, noise_stddev, n_bottleneck, index_for_statistics)`` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralnetwork_tf2 import get_prediction_from_model\n",
    "dfResults={}\n",
    "for n_bottleneck, noise_stddev in zip (n_bottlenecks, noise_stddevs):\n",
    "    encoder_name, dfResult = get_prediction_from_model(dfInput, noise_stddev, n_bottleneck, dfInput['data'].loc[dfInput['data'].index>'1900'].index)\n",
    "    dfResults[encoder_name] = dfResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import plot_hist\n",
    "for n_bottleneck, noise_stddev in zip (n_bottlenecks, noise_stddevs):\n",
    "    plot_hist(dfResults, dataset_train.index, dataset_test.index, n_bottleneck, noise_stddev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
